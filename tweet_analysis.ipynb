{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from afinn import Afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Clean tweets for sentiment analysis\n",
    "def get_clean_words(words):\n",
    "    def _isnum(w):\n",
    "        try:\n",
    "            int(w)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "        \n",
    "    # Set words to lowercase and remove them if they are stop words\n",
    "    words = [w.lower() for w in re.findall('\\w+', markup_text) if w.lower() not in stopwords]\n",
    "\n",
    "    # Remove numbers\n",
    "    words = [w for w in words if not _isnum(w)]\n",
    "        \n",
    "    # Remove hashtags\n",
    "    words = [w for w in words if !w.startswith(\"#\")]\n",
    "    \n",
    "    # Remove mentions\n",
    "    words = [w for w in words if !w.startswith(\"@\")]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(c):\n",
    "    df = pd.DataFrame()\n",
    "    divider = np.zeros(10)\n",
    "    counter = 0\n",
    "    for f in os.listdir(\"tweets/%s\" % c):\n",
    "        dft = pd.read_csv(\"tweets/%s/%s\" % (c, f))\n",
    "        divider[counter] = len(dft) - 1\n",
    "        df = pd.concat([df, dft])\n",
    "    return df, divider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "categories = [\"fast food\", \"airlines\", \"leagues\", \"colleges\", \"streaming\", \"news\", \"tech giants\", \"singers\", \"actors\"]\n",
    "\n",
    "fast_food, ff_divider = create_df(categories[0])\n",
    "airlines, al_divider = create_df(categories[1])\n",
    "leagues, lg_divider = create_df(categories[2])\n",
    "colleges, cl_divider = create_df(categories[3])\n",
    "streaming, st_divider = create_df(categories[4])\n",
    "news, nw_divider = create_df(categories[5])\n",
    "tech, tg_divider = create_df(categories[6])\n",
    "singers, si_divider = create_df(categories[7])\n",
    "actors, ac_divider = create_df(categories[8])\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_tweets = list(fast_food['text'])\n",
    "al_tweets = list(airlines['text'])\n",
    "lg_tweets = list(leagues['text'])\n",
    "cl_tweets = list(colleges['text'])\n",
    "st_tweets = list(streaming['text'])\n",
    "nw_tweets = list(news['text'])\n",
    "tg_tweets = list(tech['text'])\n",
    "si_tweets = list(singers['text'])\n",
    "ac_tweets = list(actors['text'])\n",
    "\n",
    "ff_clean = [get_clean_words(w) for w in ff_tweets]\n",
    "al_clean = [get_clean_words(w) for w in al_tweets]\n",
    "lg_clean = [get_clean_words(w) for w in lg_tweets]\n",
    "cl_clean = [get_clean_words(w) for w in cl_tweets]\n",
    "st_clean = [get_clean_words(w) for w in st_tweets]\n",
    "nw_clean = [get_clean_words(w) for w in nw_tweets]\n",
    "tg_clean = [get_clean_words(w) for w in tg_tweets]\n",
    "si_clean = [get_clean_words(w) for w in si_tweets]\n",
    "ac_clean = [get_clean_words(w) for w in ac_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn = Afinn()\n",
    "\n",
    "def get_affinity_scores(tweets):\n",
    "    scores = []\n",
    "    for t in tweets:\n",
    "        t_s = ' '.join(t)\n",
    "        scores.append(afinn.score(t_s) / len(t))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_affin = get_affinity_scores(ff_clean)\n",
    "al_affin = get_affinity_scores(al_clean)\n",
    "lg_affin = get_affinity_scores(lg_clean)\n",
    "cl_affin = get_affinity_scores(cl_clean)\n",
    "st_affin = get_affinity_scores(st_clean)\n",
    "nw_affin = get_affinity_scores(nw_clean)\n",
    "tg_affin = get_affinity_scores(tg_clean)\n",
    "si_affin = get_affinity_scores(si_clean)\n",
    "ac_affin = get_affinity_scores(ac_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affin_by_acct(tweets, divider):\n",
    "    acct = []\n",
    "    start = 0\n",
    "    for x in range(10):\n",
    "        acct.append(get_affinity_scores(tweets[start:divider[x]]))\n",
    "        start = divider[x] + 1\n",
    "    return acct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_acct = affin_by_acct(ff_clean, ff_divider)\n",
    "al_acct = affin_by_acct(al_clean, al_divider)\n",
    "lg_acct = affin_by_acct(lg_clean, lg_divider)\n",
    "cl_acct = affin_by_acct(cl_clean, cl_divider)\n",
    "st_acct = affin_by_acct(st_clean, st_divider)\n",
    "nw_acct = affin_by_acct(nw_clean, nw_divider)\n",
    "tg_acct = affin_by_acct(tg_clean, tg_divider)\n",
    "si_acct = affin_by_acct(si_clean, si_divider)\n",
    "ac_acct = affin_by_acct(ac_clean, ac_divider)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
